<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="GigaWorld-0: World Models as Data Engine to Empower Embodied AI">
  <meta name="keywords" content="GigaAI, Robotics, Embodied AI">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>GigaWorld-0: World Models as Data Engine to Empower Embodied AI</title>

  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="https://github.githubassets.com/favicons/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>

  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://github.com/open-gigaai">
        <img src="./static/images/logo.png" alt="Logo" style="height: 28px; width: auto;">
      <!-- <span class="icon">
          <i class="fas fa-home"></i>
      </span> -->
      </a>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">GigaWorld-0: World Models as Data Engine to Empower Embodied AI</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/open-gigaai">GigaAI</a></span>
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2511.19861"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- huggingface Link. -->
                <span class="link-block">
                <a href="https://huggingface.co/open-gigaai"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <img src="./static/images/Huggingface.svg" alt="Hugging Face Logo">
                  </span>
                  <span>Models</span>
                </a>
              </span>
   
              <span class="link-block">
                <a href="https://github.com/giga-world-0/giga-world-0.github.io/releases/download/v1/Gigaworld-0.mp4"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/open-gigaai/giga-world-0"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

            </div>
              <div>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!--<section class="hero video">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="hero-body">-->
<!--      <div class="content has-text-centered">-->
<!--        <video controls autoplay muted loop playsinline style="max-width: 900px; width: 100%; height: auto;">-->
<!--          <source src="./static/videos/demo.webm" type="video/mp4">-->
<!--        </video>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <figure class="image">
        <img src="./static/images/main_demo.png" alt="static Teaser" style="max-width: 900px; margin: auto;">
      </figure>
      <h3 class="subtitle has-text-centered">
        GigaBrain-0 is a Vision-Language-Action (VLA) model trained on real-world robot data and diverse data generated by world models, including video generation data, Real2Real transfer data, human transfer data, view transfer data, and Sim2Real transfer data, to enhance its generalization in real-world environments.
      </h3>
    </div>
  </div>
</section> -->

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <figure class="image">
        <video 
          src="https://github.com/giga-world-0/giga-world-0.github.io/releases/download/v1/Gigaworld-0.mp4" 
          controls 
          autoplay 
          muted 
          loop 
          playsinline
          style="max-width: 900px; margin: auto; display: block;"
        >
          Your browser does not support the video tag.
        </video>
      </figure>
      <!-- <h3 class="subtitle has-text-centered">
        GigaBrain-0 is a Vision-Language-Action (VLA) model trained on real-world robot data and diverse data generated by world models, including video generation data, Real2Real transfer data, human transfer data, view transfer data, and Sim2Real transfer data, to enhance its generalization in real-world environments.
      </h3> -->
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            World models are emerging as a foundational paradigm for scalable, data-efficient embodied AI. In this work, we present GigaWorld-0, a unified world model framework designed explicitly as a data engine for Vision-Language-Action (VLA) learning. GigaWorld-0 integrates two synergistic components: GigaWorld-0-Video, which leverages large-scale video generation to produce diverse, texture-rich, and temporally coherent embodied sequences under fine-grained control of appearance, camera viewpoint, and action semantics; and GigaWorld-0-3D, which combines 3D generative modeling, 3D Gaussian Splatting reconstruction, physically differentiable system identification, and executable motion planning to ensure geometric consistency and physical realism. Their joint optimization enables the scalable synthesis of embodied interaction data that is visually compelling, spatially coherent, physically plausible, and instruction-aligned. Training at scale is made feasible through our efficient GigaTrain framework, which exploits FP8-precision and sparse attention to drastically reduce memory and compute requirements. We conduct comprehensive evaluations showing that GigaWorld-0 generates high-quality, diverse, and controllable data across multiple dimensions. Critically, VLA model (e.g., GigaBrain-0) trained on GigaWorld-0-generated data achieve strong real-world performance, significantly improving generalization and task success on physical robots without any real-world interaction during training.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>


  </div>
</section>


<!-- Model -->
<section class="section">
  <div class="container is-max-desktop">

    <h3 class="title is-4">Model Architecture</h3>
      <div class="content has-text-justified">
        <p>
          GigaWorld-0-Video-Dreamer employs sparse attention to significantly reduce computational overhead and memory footprint during long-horizon video prediction. Inspired by DeepSeek’s Mixture-of-Experts (MoE) architecture, it further incorporates a conditional sparsity mechanism that activates only a small subset of parameters per input, enabling more efficient modeling with fewer active parameters while preserving representational capacity.
        </p>
      </div>
      <div class="content has-text-centered">
        <figure class="image">
          <img src="./static/images/framework_videodreamer.png" alt="VLA architecture">
        </figure>
      </div>

    <div>
      <h3 class="title is-4">Training efficiency</h3>
      <div class="content has-text-justified">
        <p>
        To enable efficient and cost-effective training, GigaWorld-0-Video-Dreamer adopts FP8-precision training. 
Our training infrastructure, <a href="https://github.com/open-gigaai/giga-train">GigaTrain</a>, is a unified distributed framework designed for scalability
and flexibility. It supports seamlessmulti-GPU/multi-node execution and integrates leading large-model training strategies, including: (1) Distribution framework: DeepSpeed ZeRO (Stages 0–3), FSDP2;
(2) Mixed-precision training (FP16, BF16, FP8);
(3) Gradient accumulation, gradient checkpointing, and exponential moving average (EMA);
(4) Configurable optimizers, learning rate schedulers, and other training modules.
This design enables both large-scale pretraining and resource-constrained post-training (e.g., fine-tuning with
limited compute). To facilitate community adoption, we report resource consumption for various post-training
configurations under modest hardware (e.g., 8×H20 GPUs with batch size 32), providing practical guidance
for users seeking to adapt our model and framework for downstream embodied tasks.
          <!-- FSDP-2 achieves the highest memory efficiency among the evaluated distributed training frameworks, followed by DeepSpeed ZeRO-2 and ZeRO-0, though this gain comes with higher communication overhead and slightly increased per-step latency. The table also highlights that FP8 precision consistently lowers both memory usage and training time across all frameworks, underscoring its value for scalable video foundation model training.        </p> -->
      </div>
      <div class="content has-text-centered">
        <figure class="image">
          <img src="./static/images/table_train_efficiency.png" alt="Benchmarking Pre-training">
        </figure>
      </div>
    </div>

    <br>
    <div>
      <h3 class="title is-4">Benchmark Result</h3>
      <div class="content has-text-justified">
        <p>
          We compare GigaWorld-0-Video-Dreamer against recent state-of-the-art video generation models, including Cosmos-Predict2-14B, Cosmos-Predict2.5-2B, Wan2.2-5B, and Wan2.2-14B on the PBench (Robot Set) benchmark. Despite activating the fewest parameters (2B), our model achieves the highest overall score, highlighting its superior efficiency and generation quality for embodied AI tasks.        </p>
      </div>
      <div class="content has-text-centered">
        <figure class="image">
          <img src="./static/images/table_Pbench.png" alt="Few-shot Transfer">
        </figure>
      </div>

      <div class="content has-text-justified">
        <p>
          We evaluate all models on DreamGenBench using the publicly released GR1 robot dataset and strictly follow the official fine-tuning and evaluation protocol from DreamGen, including identical hyperparameters, prompts. Even without GR1-specific data in pretraining, GigaWorld-0-Video-Dreamer consistently surpasses the comparable Cosmos-Predict2.5-2B across all three GR1 scenarios, demonstrating stronger instruction-following fidelity and controllability in embodied manipulation.        </p>
      </div>
      <div class="content has-text-centered">
        <figure class="image">
          <img src="./static/images/table_Dreamegen.png" alt="Few-shot Transfer">
        </figure>
      </div>
    </div>

  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/GigaAI-research" class="external-link">
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website page is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This website page template is borrowed from  <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
